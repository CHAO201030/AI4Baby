{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 2.0.2\n",
      "pandas 2.2.3\n",
      "torch 2.6.0+cu126\n",
      "cuda:0\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize, ConvertImageDtype\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "\n",
    "for module in np, pd, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforme = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforme\n",
    ")\n",
    "\n",
    "valid_dataset = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforme\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.7176471..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 32, 32])\n",
      "torch.Size([16])\n",
      "tensor([7, 8, 1, 1, 3, 8, 2, 3, 9, 3, 4, 8, 0, 8, 0, 4])\n",
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img, label in train_dataloader:\n",
    "    print(img.shape)\n",
    "    print(label.shape)\n",
    "    print(label)\n",
    "    print(label[0])\n",
    "    plt.imshow(img[0].permute(1, 2, 0))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopCallBack:\n",
    "    def __init__(self, patience = 5, min_delta = 0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_metric = -1\n",
    "    \n",
    "    def __call__(self, metric):\n",
    "        if metric > self.best_metric + self.min_delta:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "    \n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.activation = F.relu\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,  out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        act = self.activation\n",
    "\n",
    "        fx = self.conv1(x)\n",
    "        fx = self.bn1(fx)\n",
    "        fx = act(fx)\n",
    "\n",
    "        fx = self.conv2(fx)\n",
    "        fx = self.bn2(fx)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return act(fx + x)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)  # 32*32 -> 16*16\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)    # 16*16 -> 8*8\n",
    "        self.conv2_x = nn.Sequential(\n",
    "            ResBlock(in_channels=64,  out_channels=64,  kernel_size=3, padding=1),\n",
    "            ResBlock(in_channels=64,  out_channels=64,  kernel_size=3, padding=1)\n",
    "        )   # 8*8 -> 8*8\n",
    "        self.conv3_x = nn.Sequential(\n",
    "            ResBlock(in_channels=64,  out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            ResBlock(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        )   # 8*8 -> 4*4\n",
    "        self.conv4_x = nn.Sequential(\n",
    "            ResBlock(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            ResBlock(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        )   # 4*4 -> 2*2\n",
    "        self.conv5_x = nn.Sequential(\n",
    "            ResBlock(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n",
    "            ResBlock(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        )   # 2*2 -> 1*1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(in_features=512, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc(self.flatten(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaludating(model, valid_dataloader, loss_function):\n",
    "    loss_list = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for imgs, labels in valid_dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(axis=-1)\n",
    "        loss = loss_function(logits, labels)\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        pred_list.extend(preds.cpu().numpy().tolist())\n",
    "        label_list.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    acc = accuracy_score(label_list, pred_list)\n",
    "\n",
    "    return np.mean(loss_list), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        early_stop_cb=None,\n",
    "        eval_step=500\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    record_dict = {\"train\":[], \"valid\":[]}\n",
    "    global_step = 0\n",
    "\n",
    "    with tqdm(total=epoch * len(train_dataloader)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "            for imgs, labels in train_dataloader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(imgs)\n",
    "                loss = loss_function(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss = loss.cpu().item()\n",
    "                preds = logits.argmax(axis=-1)\n",
    "                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "                record_dict[\"train\"].append({\"loss\":loss, \"acc\":acc, \"step\":global_step})\n",
    "\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()\n",
    "                    valid_loss, valid_acc = evaludating(model, valid_dataloader, loss_function)\n",
    "                    print(f\"step : {global_step}, loss : {valid_loss}, acc :{valid_acc}\")\n",
    "                    record_dict[\"valid\"].append({\"loss\":valid_loss, \"acc\":valid_acc, \"step\":global_step})\n",
    "                    model.train()\n",
    "                    \n",
    "                    if early_stop_cb is not None:\n",
    "                        early_stop_cb(valid_acc)\n",
    "                        if early_stop_cb.early_stop:\n",
    "                            print(f\"Early Stop at step:{epoch_id} / global_step:{global_step}\")\n",
    "                            return record_dict\n",
    "                \n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"epoch_id\":epoch_id})\n",
    "    \n",
    "    return record_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071499e6cd5d41f486e671059216d83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m early_stop_cb \u001b[38;5;241m=\u001b[39m EarlyStopCallBack(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      5\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 6\u001b[0m record \u001b[38;5;241m=\u001b[39m training(model, train_dataloader, valid_dataloader, loss_function, optimizer, epoch, early_stop_cb, eval_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader))\n",
      "Cell \u001b[1;32mIn[55], line 25\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, train_dataloader, valid_dataloader, loss_function, optimizer, epoch, early_stop_cb, eval_step)\u001b[0m\n\u001b[0;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(logits, labels)\n\u001b[1;32m---> 25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ResNet().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "early_stop_cb = EarlyStopCallBack(patience=10)\n",
    "epoch = 10\n",
    "record = training(model, train_dataloader, valid_dataloader, loss_function, optimizer, epoch, early_stop_cb, eval_step=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(record_dict, sample_step=100):\n",
    "    train_df = pd.DataFrame(record_dict[\"train\"]).set_index(\"step\").iloc[::sample_step]\n",
    "    valid_df = pd.DataFrame(record_dict[\"valid\"]).set_index(\"step\")\n",
    "    fig_num = len(train_df.columns)\n",
    "    fig, axs = plt.subplots(1, fig_num, figsize=(5*fig_num, 5))\n",
    "\n",
    "    for idx, item in enumerate(train_df.columns):\n",
    "        axs[idx].plot(train_df.index, train_df[item], label=f\"train_{item}\")\n",
    "        axs[idx].plot(valid_df.index, valid_df[item], label=f\"valid_{item}\")\n",
    "        axs[idx].grid()\n",
    "        axs[idx].legend()\n",
    "        axs[idx].set_xlabel(\"step\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
