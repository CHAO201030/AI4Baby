{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib 3.10.0\n",
      "numpy 2.0.2\n",
      "pandas 2.2.3\n",
      "sklearn 1.5.2\n",
      "torch 2.6.0+cu126\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(string):\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", string) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def preprocess_sentence(string: str):\n",
    "    string = unicode_to_ascii(string.lower().strip())\n",
    "    string = re.sub(r\"([?.!,¿])\", r\" \\1 \", string)\n",
    "    string = re.sub(r'[\" \"]+', \" \", string)\n",
    "    string = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", string)\n",
    "    \n",
    "    return string.rstrip().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangPairDataset(Dataset):\n",
    "    file_path = Path(r\"./data/seq2seq/data_spa_en/spa.txt\")\n",
    "    cache_path = Path(r\"./data/seq2seq/.cache/lang_pair.npy\")\n",
    "    split_index = np.random.choice(a=[\"train\", \"valid\"], replace=True, p=[0.9, 0.1], size=118964)\n",
    "    \n",
    "    def __init__(self, mode, use_cache=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if use_cache or not self.cache_path.exists():\n",
    "            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(self.file_path, \"r\", encoding=\"utf8\") as file:\n",
    "                lines = file.readlines()\n",
    "                lang_pair = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines]\n",
    "                trg, src = zip(*lang_pair)\n",
    "                trg = np.array(trg)\n",
    "                src = np.array(src)\n",
    "                np.save(self.cache_path, {\"trg\": trg, \"src\": src})\n",
    "        else:\n",
    "            lang_pair = np.load(self.cache_path, allow_pickle=True).item() #读取npy文件，allow_pickle=True允许读取字典\n",
    "            trg = lang_pair[\"trg\"]\n",
    "            src = lang_pair[\"src\"]\n",
    "        self.trg = trg[self.split_index == mode]\n",
    "        self.src = src[self.split_index == mode]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "train_dataset = LangPairDataset(\"train\")\n",
    "valid_dataset = LangPairDataset(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_map(dataset, mode, threshold=1):\n",
    "    word2index = {\n",
    "        \"PAD\":0,\n",
    "        \"BOS\":1,\n",
    "        \"UNK\":2,\n",
    "        \"EOS\":3\n",
    "    }\n",
    "\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    index = len(index2word)\n",
    "\n",
    "    word_list = \" \".join([pair[0 if mode==\"src\" else 1] for pair in dataset]).split()\n",
    "    counter = Counter(word_list)\n",
    "\n",
    "    for word, count in counter.items():\n",
    "        if count >= threshold:\n",
    "            word2index[word]  = index\n",
    "            index2word[index] = word\n",
    "            index += 1\n",
    "    \n",
    "    return word2index, index2word\n",
    "\n",
    "src_word2index, src_index2word = get_word_index_map(train_dataset, \"src\")\n",
    "trg_word2index, trg_index2word = get_word_index_map(train_dataset, \"trg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2index, index2word, max_length=500, pad_idx=0, bos_idx=1, unk_idx=2, eos_idx=3):\n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.unk_idx = unk_idx\n",
    "        self.eos_idx = eos_idx\n",
    "    \n",
    "    def encode(self, text_list, padding_first=False, add_bos=True, add_eos=True, return_mask=False):\n",
    "        max_length = min(self.max_length, add_bos + max([len(text) for text in text_list]) + add_eos)\n",
    "        indices_list = []\n",
    "        for text in text_list:\n",
    "            indices = [self.word2index.get(word, self.unk_idx) for word in text[:max_length - add_bos - add_eos]]\n",
    "            \n",
    "            if add_bos:\n",
    "                indices = [self.bos_idx] + indices\n",
    "            \n",
    "            if add_eos:\n",
    "                indices = indices + [self.eos_idx]\n",
    "            \n",
    "            if padding_first:\n",
    "                indices = [self.pad_idx] * (max_length - len(indices)) + indices\n",
    "            else:\n",
    "                indices = indices + [self.pad_idx] * (max_length - len(indices))\n",
    "            \n",
    "            indices_list.append(indices)\n",
    "        \n",
    "        input_indices = torch.tensor(indices_list)\n",
    "        mask = (input_indices == self.pad_idx).to(dtype=torch.int64)\n",
    "\n",
    "        return input_indices if not return_mask else (input_indices, mask)\n",
    "    \n",
    "    def decode(self, indices_list, rm_bos=True, rm_eos=True, rm_pad=True, split=False):\n",
    "        text_list = []\n",
    "        for indices in indices_list:\n",
    "            text = []\n",
    "            for index in indices:\n",
    "                word = self.index2word.get(index, \"[UNK]\")\n",
    "                if rm_bos and word == \"[BOS]\":\n",
    "                    continue\n",
    "\n",
    "                if rm_eos and word == \"[EOS]\":\n",
    "                    break\n",
    "\n",
    "                if rm_pad and word == \"[PAD]\":\n",
    "                    break\n",
    "\n",
    "                text.append(word)\n",
    "            text_list.append(\" \".join(text) if not split else text)\n",
    "        return text_list\n",
    "    \n",
    "src_tokenizer = Tokenizer(word2index=src_word2index, index2word=src_index2word)\n",
    "trg_tokenizer = Tokenizer(word2index=trg_word2index, index2word=trg_index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_word = [pair[0].split() for pair in batch]\n",
    "    trg_word = [pair[1].split() for pair in batch]\n",
    "\n",
    "    # [PAD] [BOS] src [EOS]\n",
    "    encoder_inputs, encoder_inputs_mask = src_tokenizer.encode(\n",
    "        src_word, padding_first=True, add_bos=True, add_eos=True, return_mask=True\n",
    "    )\n",
    "\n",
    "    # [BOS] trg [EOS] [PAD]\n",
    "    decoder_inputs = trg_tokenizer.encode(\n",
    "        trg_word, padding_first=False, add_bos=True, add_eos=False, return_mask=False\n",
    "    )\n",
    "\n",
    "    # trg [EOS] [PAD]\n",
    "    decoder_labels, decoder_labels_mask = trg_tokenizer.encode(\n",
    "        trg_word, padding_first=False, add_bos=False, add_eos=True, return_mask=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"encoder_inputs\"        : encoder_inputs.to(device),\n",
    "        \"encoder_inputs_mask\"   : encoder_inputs_mask.to(device),\n",
    "        \"decoder_inputs\"        : decoder_inputs.to(device),\n",
    "        \"decoder_labels\"        : decoder_labels.to(device),\n",
    "        \"decoder_labels_mask\"   : decoder_labels_mask.to(device),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=1024, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, encoder_inputs):\n",
    "        '''\n",
    "            **seq_output** : [**batch_size, seq_len, hidden_dim**]\\n\n",
    "            **hidden**     : [**num_layers, batch_size, hidden_dim**]   \n",
    "        '''\n",
    "        embeds = self.embedding(encoder_inputs) # batch_size, seq_len, embedding_dim\n",
    "\n",
    "        seq_output, hidden = self.gru(embeds)\n",
    "\n",
    "        return seq_output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 1024])\n",
      "torch.Size([4, 2, 1024])\n"
     ]
    }
   ],
   "source": [
    "test_encoder = Encoder(vocab_size=100, num_layers=4)\n",
    "encoder_inputs = torch.randint(0,65, (2, 10))\n",
    "encoder_inputs.shape\n",
    "\n",
    "# Inputs : (2, 10) -> (2, 10, 256) -> (2, 10, 1024)\n",
    "# Hidden : ()\n",
    "encoder_outputs, hidden = test_encoder(encoder_inputs)\n",
    "print(encoder_outputs.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.W_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, query, keys, values, attn_mask=None):\n",
    "        \"\"\"\n",
    "            **context_vector**: [**batch_size, hidden_dim**]\\n\n",
    "            **scores**: [**batch_size, seq_len, 1**]\n",
    "        \"\"\"\n",
    "        scores = self.V(F.tanh(self.W_k(keys) + self.W_q(query.unsqueeze(-2))))\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = (attn_mask.unsqueeze(-1)) * -1e16\n",
    "            scores += attn_mask\n",
    "        scores = F.softmax(scores, dim=-2)\n",
    "        context_vector = torch.mul(scores, values).sum(dim=-2)\n",
    "\n",
    "        return context_vector, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_k.weight torch.Size([1024, 1024])\n",
      "W_k.bias torch.Size([1024])\n",
      "W_q.weight torch.Size([1024, 1024])\n",
      "W_q.bias torch.Size([1024])\n",
      "V.weight torch.Size([1, 1024])\n",
      "V.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=1024, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.attention = BahdanauAttention(hidden_dim)\n",
    "\n",
    "    def forward(self, decoder_input, hidden, encoder_outputs, attn_mask=None):\n",
    "        \"\"\"\n",
    "            **logits**: [**batch_size, seq_len, vocab_size**]\\n\n",
    "            **hidden**: [**num_layers, batch_size, hidden_dim**]\\n\n",
    "            **attention_score**: [**batch_size, seq_len, seq_len**]\n",
    "        \"\"\"\n",
    "        \n",
    "        context_vector, attention_score = self.attention(\n",
    "            query = hidden,\n",
    "            keys = encoder_outputs,\n",
    "            values = encoder_outputs,\n",
    "            attn_mask = attn_mask,\n",
    "        )\n",
    "\n",
    "        # decoder_input : [batch_size, seq_len]\n",
    "        # embeds        : [batch_size, seq_len, embedding_dim]\n",
    "        embeds = self.embedding(decoder_input)\n",
    "        \n",
    "        # embeds: [batch_size, seq_len, embedding_dim]\n",
    "        # embeds: [batch_size, seq_len, embedding_dim + hidden_dim]\n",
    "        embeds = torch.cat([context_vector.unsqueeze(-2), embeds], dim=-1)\n",
    "\n",
    "        # seq_output: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden    : [num_layers, batch_size, hidden_dim]\n",
    "        seq_output, hidden = self.gru(embeds)\n",
    "\n",
    "        # logits: [batch_size, seq_len, vocab_size]\n",
    "        logits = self.fc(self.dropout(seq_output))\n",
    "\n",
    "        return logits, hidden, attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decoder = Decoder(vocab_size=100, num_layers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            src_vocab_size,\n",
    "            trg_vocab_size,\n",
    "            encoder_embedding_dim=256,\n",
    "            encoder_hidden_dim=1024,\n",
    "            encoder_num_layers=1,\n",
    "            decoder_embedding_dim=256,\n",
    "            decoder_hidden_dim=1024,\n",
    "            decoder_num_layers=1,\n",
    "            bos_idx=1,\n",
    "            eos_idx=3,\n",
    "            max_length=512\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.max_length = max_length\n",
    "        self.encoder = Encoder(src_vocab_size, encoder_embedding_dim, encoder_hidden_dim, encoder_num_layers)\n",
    "        self.decoder = Decoder(trg_vocab_size, decoder_embedding_dim, decoder_hidden_dim, decoder_num_layers)\n",
    "\n",
    "    def forward(self, *,encoder_inputs, decoder_inputs, attn_mask=None):\n",
    "        encoder_outputs, hidden = self.encoder(encoder_inputs)\n",
    "\n",
    "        batch_size, seq_len = decoder_inputs.shape\n",
    "        logits_list = []\n",
    "        scores_list = []\n",
    "        for i in range(seq_len):\n",
    "            logits, hidden, score = self.decoder(\n",
    "                decoder_inputs[:, i:i+1],\n",
    "                hidden[-1],\n",
    "                encoder_outputs,\n",
    "                attn_mask=attn_mask\n",
    "            )\n",
    "\n",
    "            logits_list.append(logits)\n",
    "            scores_list.append(score)\n",
    "\n",
    "        return torch.cat(logits_list, dim=-2), torch.cat(scores_list, dim=-1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, encoder_input, attn_mask=None):\n",
    "        encoder_outputs, hidden = self.encoder(encoder_input)\n",
    "        decoder_input = torch.Tensor([self.bos_idx]).reshape(1, 1).to(dtype=torch.int64)\n",
    "        decoder_pred = None\n",
    "        pred_list = []\n",
    "        score_list = []\n",
    "        for _ in range(self.max_length):\n",
    "            logits, hidden, score = self.decoder(\n",
    "                decoder_input, \n",
    "                hidden[-1], \n",
    "                encoder_outputs, \n",
    "                attn_mask=attn_mask\n",
    "                )\n",
    "            decoder_pred = logits.argmax(dim=-1)\n",
    "            decoder_input = decoder_pred\n",
    "            pred_list.append(decoder_pred.reshape(-1).item())\n",
    "            score_list.append(score)\n",
    "\n",
    "            if decoder_pred == self.eos_idx:\n",
    "                break\n",
    "        \n",
    "        return pred_list, torch.cat(score_list, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_with_padding(logits, labels, padding_mask=None):\n",
    "    batch_size, seq_len, class_num = logits.shape\n",
    "    loss = F.cross_entropy(logits.reshape(batch_size * seq_len, class_num), labels.reshape(-1))\n",
    "    if padding_mask is None:\n",
    "        loss = loss.mean()\n",
    "    else:\n",
    "        padding_mask = 1 - padding_mask.reshape(-1)\n",
    "        loss = torch.mul(loss, padding_mask).sum() / padding_mask.sum()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopCallback:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = - np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    loss_list = []\n",
    "    for batch in dataloader:\n",
    "        encoder_inputs = batch[\"encoder_inputs\"]\n",
    "        encoder_inputs_mask = batch[\"encoder_inputs_mask\"]\n",
    "        decoder_inputs = batch[\"decoder_inputs\"]\n",
    "        decoder_labels = batch[\"decoder_labels\"]\n",
    "        decoder_labels_mask = batch[\"decoder_labels_mask\"]\n",
    "\n",
    "        logits, _ = model(\n",
    "            encoder_inputs=encoder_inputs,\n",
    "            decoder_inputs=decoder_inputs,\n",
    "            attn_mask=encoder_inputs_mask\n",
    "            )\n",
    "        loss = loss_fct(logits, decoder_labels, padding_mask=decoder_labels_mask)         # 验证集损失\n",
    "        loss_list.append(loss.cpu().item())\n",
    "\n",
    "    return np.mean(loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "    ):\n",
    "    record_dict = {\n",
    "        \"train\": [],\n",
    "        \"valid\": []\n",
    "    }\n",
    "\n",
    "    global_step = 1\n",
    "    model.train()\n",
    "    with tqdm(total=epoch * len(train_loader)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "\n",
    "            for batch in train_loader:\n",
    "                encoder_inputs = batch[\"encoder_inputs\"]\n",
    "                encoder_inputs_mask = batch[\"encoder_inputs_mask\"]\n",
    "                decoder_inputs = batch[\"decoder_inputs\"]\n",
    "                decoder_labels = batch[\"decoder_labels\"]\n",
    "                decoder_labels_mask = batch[\"decoder_labels_mask\"]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits, _ = model(\n",
    "                    encoder_inputs=encoder_inputs,\n",
    "                    decoder_inputs=decoder_inputs,\n",
    "                    attn_mask=encoder_inputs_mask\n",
    "                    )\n",
    "                loss = loss_fct(logits, decoder_labels, padding_mask=decoder_labels_mask)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss = loss.cpu().item()\n",
    "\n",
    "                record_dict[\"train\"].append({\n",
    "                    \"loss\": loss, \"step\": global_step\n",
    "                })\n",
    "\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()\n",
    "                    valid_loss = evaluating(model, val_loader, loss_fct)\n",
    "                    record_dict[\"valid\"].append({\n",
    "                        \"loss\": valid_loss, \"step\": global_step\n",
    "                    })\n",
    "                    model.train()\n",
    "\n",
    "                    if early_stop_callback is not None:\n",
    "                        early_stop_callback(-valid_loss)\n",
    "                        if early_stop_callback.early_stop:\n",
    "                            print(f\"Early stop at epoch {epoch_id} / global_step {global_step}\")\n",
    "                            return record_dict\n",
    "\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "            pbar.set_postfix({\"epoch\": epoch_id, \"loss\": loss, \"valid_loss\": valid_loss})\n",
    "\n",
    "    return record_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c936901e714fc0a63cf54af3ffd015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m     16\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m---> 18\u001b[0m record \u001b[38;5;241m=\u001b[39m training(model, train_dataloader, valid_dataloader, epoch, loss_function, optimizer, early_stop_callback\u001b[38;5;241m=\u001b[39mearly_stop_cb, eval_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader))\n",
      "Cell \u001b[1;32mIn[128], line 29\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, train_loader, val_loader, epoch, loss_fct, optimizer, early_stop_callback, eval_step)\u001b[0m\n\u001b[0;32m     26\u001b[0m decoder_labels_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_labels_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     30\u001b[0m     encoder_inputs\u001b[38;5;241m=\u001b[39mencoder_inputs,\n\u001b[0;32m     31\u001b[0m     decoder_inputs\u001b[38;5;241m=\u001b[39mdecoder_inputs,\n\u001b[0;32m     32\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mencoder_inputs_mask\n\u001b[0;32m     33\u001b[0m     )\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fct(logits, decoder_labels, padding_mask\u001b[38;5;241m=\u001b[39mdecoder_labels_mask)\n\u001b[0;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[130], line 30\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, encoder_inputs, decoder_inputs, attn_mask)\u001b[0m\n\u001b[0;32m     28\u001b[0m scores_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m---> 30\u001b[0m     logits, hidden, score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m     31\u001b[0m         decoder_inputs[:, i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     32\u001b[0m         hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     33\u001b[0m         encoder_outputs,\n\u001b[0;32m     34\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     37\u001b[0m     logits_list\u001b[38;5;241m.\u001b[39mappend(logits)\n\u001b[0;32m     38\u001b[0m     scores_list\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[126], line 11\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, decoder_input, hidden, encoder_outputs, attn_mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, decoder_input, hidden, encoder_outputs, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 11\u001b[0m     context_vector, attention_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m     12\u001b[0m         query \u001b[38;5;241m=\u001b[39m hidden,\n\u001b[0;32m     13\u001b[0m         keys \u001b[38;5;241m=\u001b[39m encoder_outputs,\n\u001b[0;32m     14\u001b[0m         values \u001b[38;5;241m=\u001b[39m encoder_outputs,\n\u001b[0;32m     15\u001b[0m         attn_mask \u001b[38;5;241m=\u001b[39m attn_mask,\n\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     17\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(decoder_input)\n\u001b[0;32m     19\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([context_vector\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), embeds], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[91], line 9\u001b[0m, in \u001b[0;36mBahdanauAttention.forward\u001b[1;34m(self, query, keys, values, attn_mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, keys, values, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV(F\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_k(keys) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_q(query\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))))\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m         attn_mask \u001b[38;5;241m=\u001b[39m (attn_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e16\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2266\u001b[0m, in \u001b[0;36mtanh\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   2251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m   2254\u001b[0m softshrink \u001b[38;5;241m=\u001b[39m _add_docstr(\n\u001b[0;32m   2255\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msoftshrink,\n\u001b[0;32m   2256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m,\n\u001b[0;32m   2263\u001b[0m )\n\u001b[1;32m-> 2266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtanh\u001b[39m(\u001b[38;5;28minput\u001b[39m):  \u001b[38;5;66;03m# noqa: D400,D402\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"tanh(input) -> Tensor\u001b[39;00m\n\u001b[0;32m   2268\u001b[0m \n\u001b[0;32m   2269\u001b[0m \u001b[38;5;124;03m    Applies element-wise,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.Tanh` for more details.\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mtanh()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "batch_size = 64\n",
    "\n",
    "src_vocab_size = len(src_word2index)\n",
    "trg_vocab_size = len(trg_word2index)\n",
    "\n",
    "model = Seq2Seq(src_vocab_size, trg_vocab_size).to(device)\n",
    "\n",
    "loss_function = cross_entropy_with_padding\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "early_stop_cb = EarlyStopCallback()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "record = training(model, train_dataloader, valid_dataloader, epoch, loss_function, optimizer, early_stop_callback=early_stop_cb, eval_step=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[\"step\"] for i in record[\"train\"]], [i[\"loss\"] for i in record[\"train\"]], label=\"train\")\n",
    "plt.plot([i[\"step\"] for i in record[\"valid\"]], [i[\"loss\"] for i in record[\"valid\"]], label=\"val\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self, model, src_tokenizer, trg_tokenizer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "\n",
    "    def draw_attention_map(self, scores, src_words_list, trg_words_list):\n",
    "        plt.matshow(scores.T, cmap='viridis')\n",
    "\n",
    "        ax = plt.gca()\n",
    "\n",
    "        for i in range(scores.shape[0]):\n",
    "            for j in range(scores.shape[1]):\n",
    "                ax.text(j, i, f'{scores[i, j]:.2f}',\n",
    "                               ha='center', va='center', color='k')\n",
    "\n",
    "        plt.xticks(range(scores.shape[0]), src_words_list)\n",
    "        plt.yticks(range(scores.shape[1]), trg_words_list)\n",
    "        plt.show()\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "        encoder_input, attn_mask = self.src_tokenizer.encode(\n",
    "            [sentence.split()],\n",
    "            padding_first=True,\n",
    "            add_bos=True,\n",
    "            add_eos=True,\n",
    "            return_mask=True,\n",
    "            )\n",
    "        encoder_input = torch.Tensor(encoder_input).to(dtype=torch.int64)\n",
    "\n",
    "        preds, scores = model.infer(encoder_input=encoder_input, attn_mask=attn_mask)\n",
    "\n",
    "        trg_sentence = self.trg_tokenizer.decode([preds], split=True, remove_eos=False)[0]\n",
    "\n",
    "        src_decoded = self.src_tokenizer.decode(\n",
    "            encoder_input.tolist(),\n",
    "            split=True,\n",
    "            remove_bos=False,\n",
    "            remove_eos=False\n",
    "            )[0]\n",
    "\n",
    "        self.draw_attention_map(\n",
    "            scores.squeeze(0).numpy(),\n",
    "            src_decoded,\n",
    "            trg_sentence\n",
    "            )\n",
    "        return \" \".join(trg_sentence[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(model.cpu(), src_tokenizer, trg_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(u'hace mucho frio aqui .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(u'esta es mi vida.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
