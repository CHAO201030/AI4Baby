# 机器学习

## 一、监督学习(Supervised Learning)

### 1.1 回归(Regression)

#### 1.1.1 线性回归(Liner Regression)

1.基本模型
$$
f_{w,b}(x)=\vec{w}\cdot \vec{x}+b\\\vec{x}:Input\\\vec{w},b:parameters\\ f_{w,b}(x)=\widehat{y}:Predict\ Output\\(x^{(i)},\ y^{(i)}): i^{\ th}\ Value
$$
2.代价函数(cost function)

平方误差函数
$$
J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2
$$
#### 1.1.2 梯度下降(Gradient Descent) 

1.目的：找到一组参数让损失函数最小

注意：找到的点为局部极小值点，可能不是全局最小值点

2.算法实现
$$
Repeat\ Until\ Convergence
\{\\
w_{i} = w_{i}-\alpha\ \frac{\partial{J(\vec{w},b)}}{\partial{w_{i}}}\\
b = b-\alpha\ \frac{\partial{J(\vec{w},b)}}{\partial{b}}\\
\}\\
\alpha:Learning\ Rate
$$
注意：w和b应该同时更新，使用中间变量接收然后再赋值给原变量(不使用中间变量会产生类似于数据相关的问题)

3.学习率的取值

如果 α 取值过小：由于每次步长过小，导致迭代次数变多，计算效率变差

如果 α 取值过大：由于每次步长过大，可能导致算法无法收敛，甚至发散

4.分类

(1)批量梯度下降("Batch" Gradient Descent)

每次都使用所有的训练样本来进行梯度下降算法

(2)()

(3)()

#### 1.1.3 向量化(Vectorization)

1.目的：使代码更短，运行速度更快

~~~python
# Without vectorization
f = 0
for i in range(0, n):
    f = f + w[i] * x[i]
f = f + b

# Vectorization
f = np.dot(w, x) + b
~~~

2.底层原理：指令并行

#### 1.1.4 特征缩放(Feature Scaling)

1.目的：将各个特征映射到相近的范围内， 使梯度下降算法运行速度加快

2.分类

(1)均值归一化(Mean Normalization)
$$
x_{i}=\frac{x_{i}-\mu_{i}}{x_{i\_max}-x_{i\_min}}\\
\mu_{i}=average(x_{i})
$$
特征映射出的值有正有负，映射后的特征点分布在原点附近

(2)Z-Score归一化(Z-Score Normalization)  ===>  标准化(Standardization)
$$
x_{i}=\frac{x_{i}-\mu_{i}}{\sigma_{i}}\\
\mu_{i} = average(x_{i})\\
\sigma_{i} = \sqrt{\sum_{j=1}^{n}(x_{i\_j}-\mu_{i})^2}
$$


特征映射出的值有正有负，映射后的特征点分布在原点附近

#### 1.1.5 特征工程(Feature Engineering)

1.目的：从原始数据中提取并构建特征来提高模型的准确度

#### 1.1.6  多项式回归(Polynomial Regression)

1.目的：拟合曲线

### 1.2 分类(Classification)

#### 1.2.1 逻辑回归(Logistic Regression)

1.基本模型
$$
f_{\vec{w},b}(\vec{x}) = \vec{w}\cdot\vec{x}+b=z\ ,\ 
g(z) = \frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}\\
or\\
f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}
$$
2.决策边界(Decision Boundary)

划分逻辑回归结果的分界线

3.损失函数(loss function)

注意：代价函数(cost)是对于整个训练集而言，损失函数(loss)是对于单个样本而言的
$$
L(f_{\vec{w},b}(\vec{x}^{(i)},y^{(i)})) = 
\begin{cases}
-log(f_{\vec{w},b}(\vec{x}^{(i)}))&if\ y^{(i)}=1\\
-log(1-f_{\vec{w},b}(\vec{x}^{(i)}))&if\ y^{(i)}=0
\end{cases}
$$
简化版
$$
L(f_{\vec{w},b}(\vec{x}^{(i)},y^{(i)})) = 
-y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))
-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))
$$


4.代价函数
$$
J(\vec{w}, b)=\frac{1}{m}\sum_{i=1}^{m}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})
$$
 注意：3、4的 f 特指逻辑回归的输出，即g

### 1.3 过拟合与欠拟合(Overfitting & Underfitting)

#### 1.3.1 过拟合

模型太复杂，拟合结果过于符合训练集，缺乏泛化能力，对测试集表现较差

解决方式

(1)收集更多数据(Collect more data)

(2)选择特征(Select Features)  ===>  特征选择

(3)减少参数的大小(Reduce size of parameters)  ===>  正则化

#### 1.3.2 欠拟合

模型太简单，拟合结果连训练集都不太符合，对任何数据表现都很差

#### 1.3.3 正则化(Regularization)

1.目的：通过减少参数的大小来解决过拟合现象

2.代价函数

对于线性回归
$$
J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2
+\frac{\lambda}{2m}\sum_{i=1}^{n}w_{i}^{2}\\
\lambda:Regularization\ Parameters
$$
 对于逻辑回归
$$
J(\vec{w}, b)=\frac{1}{m}\sum_{i=1}^{m}L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})
+\frac{\lambda}{2m}\sum_{i=1}^{n}w_{i}^{2}\\
$$


## 二、非监督学习(Unsupervised Learning)

### 2.1 聚类(Clustering)

 

### 2.2 异常检测(Anomaly detection)



### 2.3 降维(Dimensionality reduction)



## 三、高级学习算法(Advanced Learning Algorithms)

### 3.1 神经网络(Neural Networks)

#### 3.1.1 基本概念

1.神经网络：通过算法模仿人脑神经元的工作方式的模型

2.组成

![image-20241231151553363](C:\Users\machenike\AppData\Roaming\Typora\typora-user-images\image-20241231151553363.png)

(1)输入层(Input Layer)

(2)隐藏层(Hidden Layers)

(3)输出层(Output Layer)

3.常见类型

(1)前馈神经网络(Feedforward Neural Network)

(2)卷积神经网络(Convolutional Neural Network)

(3)循环神经网络(Recurrent Neural Network)

(4)生成对抗网络(Generative Adversarial Network)

(5)自编码器(Autoencoder)

(6)Transformer

#### 3.1.2 前向传播(Forward Propagation)

1.基本模型
$$
\vec{a}^{[i]} = [g(\vec{w}^{[i]}\cdot\vec{a}^{[i-1]}+b^{[i]})]
$$


2.基本代码实现

```python
# Tensorflow
x = np.array([[200.0, 17.0]])
x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)

layer_1 = Dense(units=3, activation='sigmoid')
layer_2 = Dense(units=1, activation='sigmoid')

a1 = layer_1(x_tensor)
a2 = layer_2(a1)

# Pytorch
x = torch.tensor([[200.0, 17.0]])

layer_1 = nn.Sequential(
	nn.Linear(in_features=2, out_features=3),
	nn.Sigmoid()
)
layer_2 = nn.Sequential(
	nn.Linear(in_features=3, out_features=1),
	nn.Sigmoid()
)

a1 = layer_1(x)
a2 = layer_2(a1)
```

3.具体实现  ===>  (Coffee Roasting Model)

```python
def sigmoid(z):
    return 1/(1+np.exp(-z))

def dense(a_in, W, b, g):
    units = W.shape[1]
    a_out = np.zeros(units)
    for j in range(units):
        w = W[:,j]
        z = np.dot(w, a_in) + b[j]
        a_out[j] = g(z)
    
    return a_out

def main():
    W1 = np.array([
    	[1, -3, 5],
    	[2, 4, -6]
	])
	b1 = np.array([-1, 1, 2])

	W2 = np.array([
        [3],
        [-2],
        [1]
    ])
	b2 = np.array([3])

	x = np.array([-2, 4])
    
    a1 = dense(x, W1, b1, sigmoid)
    a2 = dense(a1, W2, b2, sigmoid)
    
    if a2[0] > 0.5:
        print("Yes!")
    else:
        print("No!")

if __name__ == "__main__":
    main()
```

